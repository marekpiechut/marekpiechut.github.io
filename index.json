[{"content":"This post is a work in progress chapter from ‚ÄúReact Performance‚Äù book i‚Äôve started working on and haven‚Äôt touched for few months. Looks like it will never be released, so I‚Äôll just post parts that I think might be useful as blog posts._You‚Äôre probably here, because you already know that something is wrong with a part of your application. Maybe after some change, maybe when hit with a big dataset, your component/screen started to get slow. You probably can see it when you invoke some action, that UI is no longer responsive and you‚Äôre pretty sure it‚Äôs React rendering that takes too long.\nYou‚Äôre already thinking about adding some useCallback, useMemo, and React.memo calls, but where? Sometimes it\u0026quot;s obvious. Component structure is simple and you have the gut feeling what might be causing the problem. But sometimes you just stare at the code and have no idea where, in this huge component tree, we\u0026quot;re dropping these frames. Which component to wrap with memo or which callback is responsible for component re-rendering regardless of nothing changing in it\u0026quot;s data.\nQuick Win In this section we‚Äôll try to answer exactly that‚Ää‚Äî‚Äähow to find that one component (or group of components) that is causing the slowdown.\nLet‚Äôs start with quickest way we can get to the answer. React already has some nice tools that will help us out. We‚Äôll dive into React Profiler and without much introduction just try to hunt the performance hog.\nInstall React Profiler ‚Äî\nü§î Feel free to skip this section if you already have it installed.\n‚Äî\nReact profiler is a part of totally awesome React Developer Tools package available for Chrome/Chromium and Firefox. You can download it here:\nChrome: https://chrome.google.com/webstore/detail/react-developer-tools/fmkadmapgofadopljbjfkapdkoienihi Firefox: https://addons.mozilla.org/en-US/firefox/addon/react-devtools/ Both are being developed by core React team and built from the same codebase, so they should provide the same functionality. So it shouldn‚Äôt matter which one you use. Just select browser you‚Äôre most comfortable to work with. Unfortunately there‚Äôs no support for other tools as of now. You‚Äôll have to use standalone edition if you need to debug/profile these (we‚Äôll dive into that later on).\nAfter installation you should be able to access React profiler inside your browser developer tools (we‚Äôre looking for the Components and Profiler tabs):\nWhen you have it all and running open your application in the browser, navigate to Profiler tab and start profiling (Press blue circle button in the top left corner). Click around your app for few seconds and stop it by pressing red circle button . If you didn\u0026quot;t get any errors we should be all set and ready to dive into it.\n‚Äî\n‚ö†Ô∏è I‚Äôm getting ‚ÄúProfiling not supported.‚Äù message, what next?‚Ää‚Äî‚Äämake sure you‚Äôre running your React app in development mode. Profiling is disabled by default on production builds. Don‚Äôt worry for now. Will handle it later. Just switch to development mode. Most of the performance issues will be visible in both‚Ää‚Äî‚Ääproduction and development builds anyways.\n‚Äî\nInvestigate slow page Now that we have our profiler running we can go on and try to investigate some slowly loading page in your app. We‚Äôll be investigating a lazy loaded list of old notifications. We expect it to be doing more work than required when each load is finished, but we encourage you to open up application you‚Äôre currently developing and check a page that you think might need some performance improvements and it‚Äôs not related to network time.\nSo let‚Äôs dit into it. As always we start with a clean state to make sure we‚Äôre touching only code we think we do and that nothing is going to skew our measurements, so:\n‚úîÔ∏è Checkout to a commit you want to profile or stage your current changes\n‚úîÔ∏è Make sure theres not much going on on your machine. CPU or memory hungry applications might skew our results.\nAnd here‚Äôs the screen we‚Äôll be working on:\nIt‚Äôs currently showing the last 20 notifications, but we have many more on the backend and whenever you‚Äôre near the bottom we‚Äôll be fetching new ones and appending to the list. Just classic lazy loading infinite (Of course it‚Äôs not infinite, we‚Äôll run out of notifications in few screens, but it has enough to make it interesting) scroll.\nWhat we need to do now, after initial rendering of the page is done, is to gather a profiler baseline, so we know if we‚Äôre fixing performance or making it worse. Let‚Äôs open React Profiler, start recording and scroll down until we hit the lazy load boundary few times, at least twice. Then immediately stop the profiling session. Output should be similar to what you see on the screenshot below.\nThat‚Äôs a lot of output, but we‚Äôre not really interested in all of that. We start by investigating the commits chart at the top:\nWhat we can see here, is that we had 20 ‚Äúcommits‚Äù‚Ää‚Äî‚Ää20 times React has applied changes to the browser DOM. That‚Äôs not bad actually when we think about all the loading state changes and that we‚Äôve executed lazy loads few times. What is more interesting though, is that this chart is constantly moving up, so it looks like we‚Äôre doing more and more work on each commit. When you hover over the bars, you‚Äôll also see, that we get from around 90ms to almost 200ms time for single render. This is probably now what we were expecting. We are fetching 20 items in each batch and appending them to the list. Of course we‚Äôre doing some more work when going through ever-growing list of items, but we should only append new 20 nodes to the DOM tree. It looks suspicious, let‚Äôs dig into it a little bit more.\nTo do that, we start at the last commit and check what was rendered there.\nWe can ignore all the grey bars‚Ää‚Äî‚Ääthese components did not change during this render. First interesting one is the Notifications component. It did render, and that\u0026quot;s expected. We have changed its state, so this looks fine. We also see, that commit of Notifications component itself did not take a lot of time. It\u0026quot;s only 3.5ms. But take a look on all these small components at the bottom. All of them are colored, so they all have changed and their DOM changes were applied. Compare them to the previous screen. We see that in each commit there\u0026quot;s more and more of these, and all of them are always rendered. If you hover over them, you\u0026quot;ll see, that they are the individual notifications.\nAlso, if you did not refresh the page or change page structure, React Profiler will highlight actual nodes that were rendered during the commit. Scrolling to the top of the page and hovering over one of the first nodes in the flame chart will reveal the truth. We‚Äôre re-rendering every notification on each update.\nThis makes no sense. These components will never change. We only append new notifications at the end. There‚Äôs no way data can change and they should not trigger any DOM updates. It looks like we‚Äôve found good place to try out React.memo. Let\u0026quot;s wrap our NotificationItem into it and see if it helps.\nMemoize Component Digging into the source code it looks like we‚Äôre using just a regular functional component for a single notification item. Usually that‚Äôs totally fine and in most cases should not matter. But here we‚Äôre rendering quite a lot of them and it makes sense to give it few minutes of thought and try to memoize rendered component. Here‚Äôs how it look like right now. Actual body of the component does not matter, it‚Äôs only important that it is a functional component and that it‚Äôs not memoized yet. const NotificationItem = ({ notification, onMarkAsRead, session }: Props) =\u0026gt; \u0026lt;div\u0026gt;...\u0026lt;/div\u0026gt;\nOk, let‚Äôs wrap it up with React.memo and see if it helps:\nconst NotificationItem = React.memo\u0026lt;Props\u0026gt;( ({ notification, onMmarkAsRead, session }) =\u0026gt; \u0026lt;div\u0026gt;...\u0026lt;/div\u0026gt; ) Now rebuild project, and start a new profiling session. Make sure that you have similar environment (nothing heavy is running on your computer and you gave few seconds for VM to warm up). And check if our fix has helped to mitigate the problem:\nYup, looks like the problem is fixed. In the commits bar we can see a ‚Äúrake‚Äù pattern. Lower bar being a render of loading indicator, when no notification change, and higher bar being actual rendering of newly fetched data. Also each render takes similar time now‚Ää‚Äî‚Ääaround 60‚Äì70ms and looking at the flame chart at the bottom we see, that previously fetched notifications are not re-rendered (they are greyed out).\nWrap Up Looks like we have fixed our problem and here we have our quick win. It was enough to simply wrap a component with React.memo. But what\u0026quot;s more important here is that we\u0026quot;ve made an informed change. There was no guesswork about where the problem is. We had hard evidence, that there was unnecessary work being done and then we have fixed it.\n‚Äî\n‚ö†Ô∏è There‚Äôs some more consideration needed when using React.memo. Sometimes adding it adds more overhead instead of fixing the problem. It might be the case, that it won‚Äôt help in your project.\n‚Äî\nDeep dive React Profiler is a nice tool to quickly investigate possible performance issues in rendering process. In this chapter we‚Äôll be doing a deep dive into how it works and what actually it measures. We‚Äôll also take a look at React APIs that are used internally by the profiler and investigate how we might use it ourselves to get more answers than React Profiler provides.\nProfiler UI Let‚Äôs take a detailed look into information available in React Profiler and how it‚Äôs presented. To have a better image of what is going on here, we need to remember about 2 stages of React rendering process:\nFirst stage called Render or Reconciliation is one that calls all render functions on components, generates a Virtual DOM and compares it to the previous vDOM. This way it knows which nodes have changed and need to be applied to the browser DOM. Second stage‚Ää‚Äî‚ÄäCommit‚Ää‚Äî‚Ääresponsible for doing actual changes on browser DOM. This has to be done in single call to the browser APIs and will be done in ‚Äústop the world‚Äù manner. If we would allow it to be done in stages or in background, user would see intermediate stages while we are updating the UI. With that in mind we can now take a look at the commits view in the profiler. It‚Äôs this small bar chart at the top:\nWhat we see here is a list of all commits that React has flushed to DOM during our profiling session. Each bar is showing a separate commit in sequence of execution. Although it shows commits, it does count the render phase. You can test it out using this simple code:\nimport { useState } from \u0026#34;react\u0026#34; const SlowComponent = ({ noSlowdown }) =\u0026gt; { const arr = [] if (!noSlowdown) { for (var i = 1000000 - 1; i \u0026gt;= 0; i--) { arr.push(i) } } return \u0026lt;div\u0026gt;I\u0026#34;m slooooooow\u0026lt;/div\u0026gt; } const FastComponent = () =\u0026gt; { return \u0026lt;div\u0026gt;I\u0026#34;m fassssst....\u0026lt;/div\u0026gt; } const App = () =\u0026gt; { const [dummy, setDummy] = useState() return ( \u0026lt;div className=\u0026#34;App\u0026#34;\u0026gt; \u0026lt;SlowComponent noSlowdown={dummy % 2} /\u0026gt; \u0026lt;FastComponent /\u0026gt; \u0026lt;button onClick={() =\u0026gt; setDummy(Date.now())}\u0026gt;Render!\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; ) } export default App Loading it and pressing Render! button few times shows that slow component is impacting profiling results regardless of it returning the same trivial single div. Commit cost of both components is exactly the same, yet profiling results are completely different: fast component renders nearly instantly while slow component takes 32ms out of 33ms total rendering time.\nTraveling back and fourth through the commits you can find some interesting patterns in render times that you didn‚Äôt expect. Selecting one of them will show you exact render duration and few more details about it.\nAs you have probably already noticed React Profiler marks commits and components with different colors and bar length. Generally the longer the bar and warmer the color (more yellow), the more time it took to render compared to other commits and components. Grey lines did not render at all during given run.\n‚Äî\nü§î Note:\nLine length shows how long did it take for component to render with all it‚Äôs children Line color shows how long did component itself render compared to other components (how slow/fast it is) ‚Äî\nThere are two more interesting views here: Flamegraph and Ranked tab. First shows all components in your view in a tree according to your components structure. Second orders components by their rendering time ( self render time‚Ää‚Äî‚Äähow long it took to render component excluding its children time). It‚Äôs very useful to catch slowest components‚Ää‚Äî‚Ääa good place to start searching for problems. Hovering on the bars will highlight it on the page and selecting one of them will show you history of commits of given components and reason why render was triggered (ex: state change, parent render, props change, etc.). This information is sometimes enough to get you on track with figuring out what‚Äôs wrong. Very often some props change, that we didn‚Äôt expect to be any different.\n‚Äî\n‚ö†Ô∏è It‚Äôs usually good idea to have ‚ÄúRecord why each component rendered while profiling.‚Äù option enabled. It might add some overhead to profiling process, but it‚Äôs usually not a problem if your app is not super-huge. Just press the options cog ‚öôÔ∏è and enable it now.\n‚Äî\nThere‚Äôs also an option to hide commits that took below given time. It‚Äôs not super useful, as we usually are looking for anomalies, and want to see whole picture. It‚Äôs better idea to narrow use case to short problematic interaction, but you might also start with longer recording sessions and try to find issues using this filter.\nReact Profiler API DevTools profiler allows to gain a lot of knowledge about performance issues in your app. But if what you need is to profile only selected component and do it in production, there‚Äôs also a React Profiler API (https://reactjs.org/docs/profiler.html). It‚Äôs a lightweight component to gather data about it render times, thus giving an insight of performance of its children.\nIt will not allow us to get a deep tree of render stats like we have in Profiler view in DevTools, but allows to programmatically get timings for selected components in the app. Usage is simple, just wrap your component and provide onRender callback:\n\u0026lt;Profiler id=\u0026#34;user-profile\u0026#34; onRender={( id, phase, actualTime, baseTime, startTime, commitTime, interactions ) =\u0026gt; aggregatePerformance(id, phase, actualDuration, interactions)} \u0026gt; \u0026lt;UserProfile user={user} /\u0026gt; \u0026lt;/Profiler\u0026gt; We‚Äôve got quite a few parameters to the callback, but except of that it looks pretty straightforward. Let‚Äôs go through all the data we have in the callback:\nid‚Ää‚Äî‚Ääthis is exactly the same as what we have passed into the Profiler tag. React does nothing interesting with this parameter. It\u0026quot;s not grouping or overriding profiler tags with same Id. You can have as many as you want with exactly same value. Its role is purely informational, so you can have single callback, reuse it between profilers and still know which measures you have received. phase‚Ää‚Äî‚Ääcomponent lifecycle phase. This can only be: mount or update and means exactly what you expect. Can be used find out if problem is only appearing during mount - maybe some slow useEffect that is firing only on mount. actualTime‚Ää‚Äî‚Äämeasured duration of rendering. This is a real time of render during given commit. If you‚Äôre using memoization this time should go down significantly. If it does not, then you might found the problem. baseTime‚Ää‚Äî‚Ääestimated time of full render of component subtree. This should roughly be the same time as the actualTime when component did mount if it did not do any heavy initialization. If you‚Äôre using memoization correctly, this time should be usually larger. startTime‚Ää‚Äî‚Ääsimply a timestamp when rendering started commitTime‚Ää‚Äî‚Äätimestamp when rendering was finished. This time is shared between all profilers that took part in the commit. Thanks to that you can group data from separate sub-trees that were rendered due to same change in state. interactions‚Ää‚Äî‚Äälist of all interactions associated with this render. Profiler API is very simple and allows to track performance of our components even on production. Just keep in mind, that it has a cost associated. There‚Äôs a small performance penalty for each Profiler instance in your components tree. Also make sure you\u0026quot;re not doing a lot of processing in your callback. It\u0026quot;s best to do any calculations asynchronously, or just dump your data to server and do all heavy-lifting there.\nProfiling in production builds Due to small performance impact profiler API is disabled in production builds of React (as of May 2021). While it‚Äôs usually not a problem, as most of real performance issues will be visible in both production and development builds. Sometimes you want to dig into production bundle and gain some insights on rendering time.\nThe way it‚Äôs currently being disabled is through build time configuration. So if you have built your app with regular ReactDOM, you‚Äôll have to re-build it. Here‚Äôs what to put in Webpack config to switch to profiling versions of libraries:\nmodule.exports = { //... resolve: { alias: { \u0026#34;react-dom$\u0026#34;: \u0026#34;react-dom/profiling\u0026#34;, \u0026#34;scheduler/tracing\u0026#34;: \u0026#34;scheduler/tracing-profiling\u0026#34;, } } }; In case you‚Äôre using create-react-app script, just add --profile flag during build process:\n# Using Yarn yarn build --profile # Using NPM npm run build -- --profile With all this in place profiling should work just fine in production build. Regardless of that we‚Äôre not really advising to add it to your build process. It should be enough to run this build locally and connecting it to production backend. It should be trivial if you‚Äôre already serving your app from CDN or static folder using your Web server.\nPerformance impact As we have already noticed, profiler builds tend to run slower. Even if you‚Äôre not actively profiling. We‚Äôve done a quick benchmark to actually measure it:\nconst ROUNDS = 100_000 const Benchmark = () =\u0026gt; { const [took, setTook] = useState() const [round, setRound] = useState(1) const start = useRef(performance.now())``useEffect(() =\u0026gt; { if (round \u0026lt; ROUNDS) { setRound(round + 1) } else { setTook(performance.now() - start.current) } }, [round])``return ( \u0026lt;div className=\u0026#34;App\u0026#34;\u0026gt; \u0026lt;div\u0026gt;{round}\u0026lt;/div\u0026gt; \u0026lt;div\u0026gt;Took: {took}ms\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ) } And results are:\nDevelopment: 13575ms\nProduction: 1478ms\nProduction Profiling: 2179ms\nSo it looks like performance hit is not that bad, but it‚Äôs still there. We wouldn‚Äôt recommend to just push profiling bundles to production. Just build one when needed and profile on your development machine or build it each time and deploy alongside your production build on separate url. This way you can always switch to profiling build and do your analysis while not sacrificing your users performance.\nInteractions ‚Äî\nüß® Watch out, this is an experimental API and might change in future versions of React\n‚Äî\nAlong with the profiler API React team has introduced an interaction tracking that can be attached to profiling data. Sometimes it‚Äôs just hard to find out which exact user action or backend call ended up with very slow render commit. What this API allows, is to attach a bit of semantic context to recorded performance traces.\nSometimes we can track the component that is slow, and tack slow sessions, but it‚Äôs not always easy to find out which exact user interaction is causing this. Especially when we are trying to gather performance data remotely, from user machine, as it‚Äôs not showing up on our environment. Thanks to the interactions API we‚Äôre able to see exactly which user actions are slow and which are fast, even if they are triggering the same component or we don‚Äôt exactly know where to look for the problem.\nWhat we can see on the screen above, is a trace of user interactions on the same component. Based on grid color we can see, that ‚ÄúEnter user name‚Äù action is pretty fast. What we should be looking at is the ‚ÄúRandomize button‚Äù action. Its marker color is yellow and render duration is much bigger. Also when looking from the commit perspective it can be found, that this action is problematic:\nAnd here‚Äôs how we track interaction in this example:\nimport { unstable_trace as trace, } from \u0026#34;scheduler/tracing\u0026#34;``\u0026lt;input type=\u0026#34;text\u0026#34; value={name} onChange={e =\u0026gt; { trace(\u0026#34;Enter user name\u0026#34;, performance.now(), () =\u0026gt; onChange(e.target.value) ) }} /\u0026gt; What we do here, is on each character entered in the input we call trace first, passing in high resolution timestamp and a callback to handle actual logic after interaction is recorded. Don\u0026quot;t worry about the performance object. It\u0026quot;s a global available in every major browser now. We use it here, because Date.now() is not good enough to measure code performance. You can read more about it in your browser documentation, ex: performance-mdn.\nThere‚Äôs one more thing we‚Äôre missing here. It‚Äôs how to add interactions to asynchronous code. A lot of performance issues are related to data we‚Äôre fetching from backend. If we use this simple trace API, our async code calls will not be counted as part of the interaction. That\u0026quot;s simply because of the way async is handled in JS. What we need to do then is to wrap callbacks of async code with container that will bind it to current interaction. Thankfully React already has tools to support it:\nimport { unstable_trace as trace, unstable_wrap as wrap, } from \u0026#34;scheduler/tracing\u0026#34;``\u0026lt;button onClick={() =\u0026gt; trace(\u0026#34;Fetch data\u0026#34;, performance.now(), () =\u0026gt; { setLoading(true) fetchData() .then( wrap(data =\u0026gt; { setData(data) }) ) .finally(wrap(() =\u0026gt; setLoading(false))) }) } \u0026gt; Fetch data \u0026lt;/button\u0026gt; What‚Äôs most important here, is the wrap call that is surrounding then and finally handlers in our async call. After that we should see, that all renders related to this call are assigned to the same interaction:\nWe can see here, that ‚ÄúFetch data‚Äù has exactly 3 renders associated. This totally makes sense. First one is sue to loading being set to true, second when we finish fetching the data and third when loading is set back to false. Of course all this code does not need to be inlined in the component. Interaction tracing API can as well be used inside custom hooks:\nimport { unstable_trace as trace, unstable_wrap as wrap, } from \u0026#34;scheduler/tracing\u0026#34;``const useData = () =\u0026gt; { const [loading, setLoading] = useState(false) const [data, setData] = useState(null)``const fetch = () =\u0026gt; { setLoading(true) fetchData() .then( wrap(data =\u0026gt; { setData(data) }) ) .finally(wrap(() =\u0026gt; setLoading(false))) } return { fetch, loading, data } }``const Component = () =\u0026gt; { const { fetch, loading, data } = useData()``return \u0026lt;button onClick={() =\u0026gt; trace(\u0026#34;Fetch data\u0026#34;, performance.now(), fetch)} \u0026gt; Fetch data \u0026lt;/button\u0026gt; } Details and Techniques In this chapter we take a look at other cases (except our ‚ÄúQuick Win‚Äù) that make React Profiler worth while. We also talk about cases where it‚Äôs not very useful and what to do if we cannot get any meaningful input from it.\nUsing profiler in other browsers While Chrome is the leading browser (at least as of 2021) we sometimes also need to support clients using other agents‚Ää‚Äî‚Äänoticeable ones being Safari and mobile safari. For this we‚Äôll be using standalone version of React Dev Tools and connect it remotely:\nTo get standalone tools fetch it with yarn or npm and simply start: npm install -g react-devtools react-devtool\nWhat you should get is a welcome screen with information about how to connect. For browsers it should be as easy as adding a script to your page \u0026lt;head\u0026gt; section: \u0026lt;script src=\u0026quot;[http://localhost:8097](http://localhost:8097)\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\nand reloading the page. This will open a web-socket connection from your browser to the app used for all communication between debugger/profiler and dev-tools.\nUnfortunately it has to run before ReactDOM is loaded, so it cannot be added in dev-tools when page is already shown in your browser. Just make sure you remove it from the code before committing or maybe you already have a separate html template for development and production. Then just put it only in dev one.\nAfter a few seconds your app should connect and you should be able to use the same tools as in Chrome extension, but within unsupported browsers.\nThis solution will also work just fine for iOS Simulator. For Android emulator/device you‚Äôll need another step‚Ää‚Äî‚Ääconnect phone using cable and proxy all requests to localhost from your device to host: adb reverse tcp:8097 tcp:8097\nUnfortunately there‚Äôs no easy way to use it with real iOS device. Due to security considerations React Dev Tools listen only on localhost and there‚Äôs no equivalent for adb reverse for iOS.\nUnexpected renders of memoized components React.memo is usually treated as a holy grail and universal tool to fix all your performance problems. But sometimes it simply doesn\u0026quot;t seem to work. Usually it\u0026quot;s due to misunderstanding of how it works or what is really passed into component props. Let\u0026quot;s try to investigate such case using React Profiler and what to look for when we suspect memoization is not working. Here\u0026quot;s our suspected component:\nconst SlowComponent = React.memo(({ text, onClick }) =\u0026gt; { const arr = [] for (var i = 1000000 - 1; i \u0026gt;= 0; i--) { arr.push(i) } return ( \u0026lt;div\u0026gt; \u0026lt;div\u0026gt;I\u0026#34;m slow {text}\u0026lt;/div\u0026gt; \u0026lt;button onClick={onClick}/\u0026gt; \u0026lt;/div\u0026gt; ) }) It looks like it‚Äôs being correctly optimized. But let‚Äôs use it in a way that simply passes an inline function as a onChange parameter. This will basically make out React.memo useless, but what we want to do is to check how will this error manifest itself in React Profiler.\nconst App = () =\u0026gt; { const loadData = id =\u0026gt; ({ id, value: \u0026#34;some\u0026#34; }) const [_, setDummy] = useState()``return ( \u0026lt;div className=\u0026#34;App\u0026#34;\u0026gt; \u0026lt;button onClick={() =\u0026gt; setDummy(Date.now())}\u0026gt;Render\u0026lt;/button\u0026gt; \u0026lt;SlowComponent onClick={() =\u0026gt; loadData(\u0026#34;dummy\u0026#34;)} /\u0026gt; \u0026lt;/div\u0026gt; ) } To investigate problem like this, we need to have at least 2 renders of parent component without any changes in data passed to SlowComponent. In this example it\u0026quot;s actually easy. We\u0026quot;ll just press a button few times and it should re-render the App component. The only prop we pass in is this problematic inline function. Thanks to React.memo it should never re-render. The only moment this component should show up colored in the profiler is its initial mount. Let\u0026quot;s see how it actually behaves in the profiler:\nWe can see here, that it has been rendered on each button press, and it has taken quite a lot of time. Thanks to the popup you also see, that it‚Äôs due to onClick property changing. This is not what we wanted here, let\u0026quot;s fix this inline function and check how will profiler output look like afterwards. Wrapping it in a simple useCallback should be enough here:\nconst App = () =\u0026gt; { const loadData = id =\u0026gt; ({ id, value: \u0026#34;some\u0026#34; }) const [_, setDummy] = useState()``return ( \u0026lt;div className=\u0026#34;App\u0026#34;\u0026gt; \u0026lt;button onClick={() =\u0026gt; setDummy(Date.now())} /\u0026gt; \u0026lt;SlowComponent onClick={React.useCallback(() =\u0026gt; loadData(\u0026#34;dummy\u0026#34;), [])} /\u0026gt; \u0026lt;/div\u0026gt; ) } And here‚Äôs how profiling results look like:\nSlowComponent did not render anywhere except of initial mount. That\u0026quot;s the result we have expected and it is clearly shown in the profiler data. This example might be trivial, but when investigating performance of theoretically optimized components you\u0026quot;ll be able to easily see them re-rendering in the profiler. Hovering on the component will also show why it has rendered. With this information it should be fairly easy to fix.\nUsing update highlights to find unneeded renders There‚Äôs one more quick way to pinpoint components like that. It‚Äôs not really a profiler feature, but React Developer Tools have also an option to mark all rendered components with color borders. It‚Äôs especially useful to track forms re-renders. Often we‚Äôll see some undesired renders when we were hoping that only single input was updated. Let‚Äôs enable the option now and check how it looks like.\nIn the screen below we can see a 3 input form. We‚Äôre editing first text input with ‚ÄúHighlight updates when components render‚Äù option enabled:\nAs you can see, on each character we put into the ‚ÄúWhat‚Äôs your name?‚Äù box, we have a render on all inputs and parent component. Parent re-rendering is probably fine, as it‚Äôs the place where we keep the state for the form. But all other inputs are not really related to the change, so we can optimize it.\nIn this particular case it might be an overkill to do it just for 2 inputs. But, especially when working with redux you might see some updates that are totally unexpected. Enabling ‚ÄúHighlight updates when components render‚Äù can be the only thing that you need to find performance issues then. It might be worth to enable it from time to time and play around with your app. You might be surprised how many unneeded renders are there.\n","permalink":"https://marekpiechut.github.io/post/2021-06-30_investigate-poor-performance-react-components-with-react-profiler/","summary":"This post is a work in progress chapter from ‚ÄúReact Performance‚Äù book i‚Äôve started working on and haven‚Äôt touched for few months. Looks like it will never be released, so I‚Äôll just post parts that I think might be useful as blog posts._You‚Äôre probably here, because you already know that something is wrong with a part of your application. Maybe after some change, maybe when hit with a big dataset, your component/screen started to get slow.","title":"Investigate poor performance React components with ‚ÄúReact Profiler‚Äù"},{"content":"Introduction ‚å®Ô∏è Typed with: Corne MX with Gateron Brown \u0026amp; BM40 with ultra-light Gateron Clear\nThis is a build guide for Corne Keyboard (Crkbd) Cherry MX keyboard version 3.0.1 by foostan. Original Japanese guide is available here. Unfortunately there‚Äôs no official guide in English for this particular version, but I‚Äôve been recently building one of them, so here it is. Please keep in mind that this version is not a translation and I don‚Äôt have even slightest Japanese knowledge. So all what‚Äôs written here is based on my own experience and might differ from the official guide.\nKeyboard done\nBefore you start Building a keyboard is not very difficult if you have previous experience with soldering or at least some tinkering skills and messed a bit with electronics. But if you have never soldered or played with electronics it might be best to pick up a YouTube tutorial on soldering first and play with some throwaway electronics you have. It‚Äôs possible in the beginning to overheat components and even destroy PCBs.\nBefore we start there‚Äôs one remark on my side. Please note that you‚Äôre the only responsible person for all damage you do while following this guide. It‚Äôs not my fault if you break anything or burn yourself. If you are not sure you can handle stuff described here on your own, don‚Äôt do it. Just order an assembled keyboard or find some qualified electronic engineer to do it for you.\nRequired equipment Soldering iron This has to be a pencil type soldering iron for small electronics with a precision tip. Don‚Äôt even try with a soldering gun. If you don‚Äôt have one I‚Äôd suggest that you get a basic soldering station with temperature setting. It will be much easier to work on LEDs if you can lower the temperature. It should not be much more expensive. If you‚Äôre going to get one, buy one with more power. It will heat up faster and thanks to temperature setting it will not fry things up.\nSolder wire To make things easy get a thin one (0.5mm is best). Don‚Äôt be very cheap on it. Poor quality solder wire will make you swear a lot, while working with high quality one is a pleasure.\nSoldering flux Just get a ‚Äúno clean SMD soldering flux‚Äù liquid. The smallest amount available. It can be a pen or a bottle with tiny brush.\nDesoldering pump Desoldering pump It‚Äôs not really required if you don‚Äôt do any mistakes. But if you do‚Ä¶ you know‚Ä¶ get it or you‚Äôll have hard times.\nPrecision tweezers‚Ää‚Äî‚Ääreverse You can also try using straight ones if you already have them. You don‚Äôt need anything fancy. We won‚Äôt be doing any surgery here. Don‚Äôt start without them, using only your fingers with SMD components is a bit too much.\nBlack insulation tape You know‚Ä¶ To insulate things.\nIsopropyl alcohol (optional) To cleanup the board of the excessive flux. Not really needed if you have used a no clean one. It will leave small stains, but nothing major. If you‚Äôre using a case it won‚Äôt really be visible.\nBlack marker (optional) Just a waterproof black marker (or rather of a color matching your PCB)\nThick sand paper (optional) Required parts There are two ways to get required parts‚Ää‚Äî‚Ääget a full kit or assemble all parts yourself. It might be easier to just order a kit and have everything done for you. Getting everything on your own is not that difficult, but can take a bit more time. You can grab gerber files from here and order PCBs at one of Chinese suppliers (ex: https://jlcpcb.com). Rest of the parts should be available on Aliexpress or your local electronics shop. This is mostly common stuff. Maybe Pro Micros or OLEDs won‚Äôt be there. Here‚Äôs the stuff you need:\nSwitches \u0026amp; Keycaps You need to pick correct type of switches for your build. They need to be MX compatible (so low profile ones won‚Äôt fit). If you are using a case with top cover you can use 3 or 5 pin switches. If you‚Äôre going to go without a case please select 5 pin ones. They will fit firmly into the board while 3 pin ones might disconnect and wobble around. There are also some switches that have a hole for LEDs. These will work best if you plan on using per-key lights.\nYou will be ok with anything that‚Äôs compatible: Gateron, Cherry, Kailh, Blue, Brown, Clear, Red, whatever. Just pick what you like, or get a switch tester and try them out before purchase.\nFor the keycaps you should be fine with anything that fits your taste and is MX compatible. Just pick something that looks cool.\nFinal notes Take a deep breath, do it well rested (I know, I know, you‚Äôll be doing it through the night regardless of what I say) and let‚Äôs get started. Just please think twice and solder once. Some mistakes are easy to fix and some will take time. I‚Äôll be adding remarks on how to fix mistakes and how hard it is to do. But it‚Äôs always faster and easier to not do them the first time. It‚Äôs also a good idea to read whole guide at least once before soldering anything, and then start following it step by step. This way you‚Äôll know what should be the expected outcome before doing actual work.\nPrepare PCB Unpack your PCBs and check for defects. It‚Äôs pretty rare that they‚Äôll be not ok, but inspect them to find some obvious stuff (you know, like broken in half or something). You will have 2 different pieces, one for left and one for right hand part. You can start with any. Just remember that side with white markings is the bottom.\nIf you want to have nice, black edges on your PCB, use sand paper and remove leftovers of PCB breakaway connectors (small bumps on the edge), clean it up with alcohol and paint the edge with permanent marker. This way it will look better, especially in dark case.\nNow cover your desk to not burn any holes in it and let‚Äôs get to the real work.\nSoldering diodes Temperature: 300‚Äì350‚ÑÉ\nDifficulty: medium\nHow hard to fix: medium\nSetup your soldering iron to 300 ‚ÑÉ, wait for it to warm up and test on the soldering wire if it melts. Give it few more minutes if it doesn‚Äôt. Low power soldering irons need at least 5 minutes to heat up. If it‚Äôs still not melting increase temperature a bit and try again. Maybe you got a poor quality soldering wire, or temperature setting is not calibrated. As a rule, try to use lower temperatures whenever possible. This gives you more time before you overheat SMD components.\nDiodes are these small black thingies that allow current flow in only one direction. There‚Äôs only one important thing here: ‚ö†Ô∏è diodes are polarized. You must solder them that line on diode is on the side with line on the PCB. Arrow points into direction of the white line on the diode. Please check twice before soldering the first one and re-check from time to time. Re-soldering one is not a problem, but redoing all 42 will not be fun.\n‚Äî\nü§î Advice: The best way to solder these for me is to first apply flux on contacts on the board. Then cover one contact for given diode with just a bit of solder. After that I would heat up the solder on the PCB until it melts and slide diode into it using tweezers.\n‚Äî\nIt‚Äôs faster to do it in batches. So, after getting comfortable with it, apply flux on 6 contacts, cover 6 with solder and solder 6 diodes on one side. Rotate the board and solder second contact in all 6. Just repeat until you‚Äôre done.\n‚Äî\nüí£ How hard to fix: diodes are not hard to fix or replace. They are also very cheap. If you get one wrong it should be 30 seconds to get it right. Just don‚Äôt get them all wrong.\n‚Äî\nSoldering per-key LEDs Temperature: 250‚Äì280 ‚ÑÉ\nDifficulty: medium\nHow hard to fix: medium\nPer key LEDs are easy to solder. They have a nice hole on the PCB, so they are easy to position and you don‚Äôt need to hold them in place while soldering. The only real issue is that they are heat sensitive. Just make sure you‚Äôre soldering with lowest possible temperature and don‚Äôt heat them longer than for 5 seconds. It should be more than enough. If you need to give it more time due to a mistake or something, just make a 30 seconds break and let it cool down. Also try to solder them near the leg end, not near the led itself. It should be enough to touch it for a second with a bit of solder on tip of your soldering pen. Just use flux and it should work.\nThey are also positional components, so make sure you get them right. There‚Äôs one shorter leg that is connected to spot with a small cutout. Shorter leg goes to the corner marked on the board and emitting part (transparent) goes into the hole‚Ää‚Äî‚Ääpointing towards the top of the PCB. They should fit and not move around when in place.\n‚Äî\nü§î Advice: My advice here is to just drop few LEDs in correct spots with tweezers, cover connectors with a bit of flux, press it gently with tip of tweezers and solder one side (2 legs) one by one. It should be a quick action, get some solder on tip of your soldering pen and touch end of the leg. Don‚Äôt worry, if you won‚Äôt get it right the first time, you‚Äôll see it after connecting the keyboard and it will be easy to fix. Finish whole board on one side, rotate PCB and do the same on the other pair of legs.\n‚Äî\nAgain, it‚Äôs faster to do it in batches. Drop all LEDs into one side, cover 2 rows with flux and solder. Cover next 2 rows, solder‚Ä¶\nJust as a reminder, these components are heat sensitive. Use high quality soldering wire and low temperature. Don‚Äôt heat it up for long, take a break and let it cool down.\n‚Äî\nüí£ How hard to fix: LEDs are a bit harder to fix. You‚Äôll need to use sucking pump, heat up solder and remove it from pins before pulling it off.\n‚Äî\nSoldering underglow LEDs Temperature: 250‚Äì280 ‚ÑÉ\nDifficulty: medium+\nHow hard to fix: medium\nThe only reason this step is marked harder is that these LEDs don‚Äôt have long legs. This makes them a bit harder to solder correctly and easier to overheat. I hope you‚Äôve got few spares, as there‚Äôs a chance you‚Äôll break one or two. Except of that it‚Äôs similar to the per-key LEDs.\nThese are also positional components and are also heat sensitive. On one corner of the LED there‚Äôs a marker or a cut corner. This spot has to go into the white corner on the board.\n‚Äî\nü§î Advice: Cover connectors with flux first, then position the LED. Press it firmly with tip of the tweezers so it doesn‚Äôt move. Then solder only one leg and move to next LED so it has time to cool down.\n‚Äî\nThese I would do one by one, each pin at a time. So solder the first leg, move to next LED, solder the first leg, etc. Finish whole board with only one leg in each then move to second leg. This way you give them some time to cool down. Also please make sure you‚Äôre not touching them with your soldering iron for more than 5 seconds. Just take a short break if you need to retry. It will cool down in 30 seconds and you can try again.\n‚Äî\nüí£ How hard to fix: For me these are a bit easier to replace than per-key ones. Just heat up each leg and suck solder with pump. It should be easier to have a firm catch with tweezers on these.\n‚Äî\nSoldering hotswap sockets Temperature: 300‚Äì350 ‚ÑÉ\nDifficulty: easy\nHow hard to fix: medium\nAfter all the LEDs and diodes, with their overheating problems, low temperature and tiny size, this will be an easy task. Sockets are big, hard to overheat and easy to position. The only thing to watch out for is to have them in place while soldering. Just press them firmly into the board and keep it pressed while soldering. You can even do it with your finger. This way you also make sure you won‚Äôt overheat them. If your finger burns, it means you‚Äôre too slow ü§™.\n‚Äî\nü§î Advice: Press sockets with your finger or tweezers when soldering, to make sure they are in place.\n‚Äî\nStart by dropping all sockets in place for whole board. Apply some flux on few of them and put your soldering iron tip with some solder straight into the hole over the PCB connector. Theres a small cut in the metal part of the socket, so solder will get through it and join it to the board. If you can‚Äôt get enough solder this way, just add it while soldering. Theres a lot of space now.\nFinish the whole board with one side of the socket, then rotate the PCB and repeat on the other side. You should be done in no time.\n‚Äî\nüí£ How hard to fix: Not very hard, just use solder pump and pull it off while heating. But there should be no need to do it anyway. They‚Äôre hard to break.\n‚Äî\nSoldering Pro Micro pins Temperature: 300‚Äì350 ‚ÑÉ\nDifficulty: easy\nHow hard to fix: VERY HARD!\nThis part is a bit tricky. It‚Äôs pretty easy to solder in the pins. They tend to sit firmly in the board and are impossible to overheat. But please make sure you solder it on the correct side of the board. Pins are cheap, but if you don‚Äôt have a hot air soldering station, they are real pain to remove. So make sure you have flipped the PCB to the other side. Pro Micro goes to the top of the board. All other parts we have been soldering till now were on the bottom. This is the side you want to be looking at:\nNow put the pins in and flip the board. It might need a bit of force to get them in place, as we are inserting square pins into round holes ü§™.\nWe‚Äôll be soldering on the bottom, but controller itself goes to the top. Please take a 5 minute break and double check that you have it right. This is really important.\n.\n.\n.\n‚è± After 5 minute break‚Ä¶\nNow check again if you have everything right, apply some flux on both pin lanes, grab your soldering iron in one hand and soldering wire in second and solder each pin individually.\n‚Äî\nü§î Advice: It‚Äôs easiest to heat up the pin and apply wire so that it touches both: pin and iron tip. This way it will melt quickly and go to right spot. This should be like a walk in the park after all the stuff we‚Äôve already done.\n‚Äî\nThis is how it should look like from the bottom after soldering (sorry I don‚Äôt have a photo of the pins before soldering controller). You might also like to socket the Pro Micros instead of soldering them in. The Micro-USB connector on them is really fragile. Its soldered only on the surface and easy to break. I wanted mine to be as low as possible and couldn‚Äôt get sockets that were low enough. Also I am using a magnetic connector anyway, but maybe socketing is an option for you. Here is a guide how you might want to do it.\n‚Äî\nüí£ How hard to fix: ‚ö†Ô∏è Watch out here. It‚Äôs really hard to desolder these if you don‚Äôt have a hot air station. You will spend quite some time fighting it, and then will have problems to fit new socket into the board, as it‚Äôs hard to clean the holes from all the solder. So double check and then check it once more. To make sure you have everything placed correctly before soldering.\n‚Äî\nSoldering TRRS socket and reset switch Temperature: 300‚Äì350 ‚ÑÉ\nDifficulty: easy\nHow hard to fix: medium\nYou can start with one you like. They are both quite easy. The only thing that can go wrong is that you solder them on the wrong side of PCB. They both go on the top side‚Ää‚Äî‚Ääthe same side as the microcontroller sockets. You also can‚Äôt solder them the wrong way. Tactile switch is not positional and TRRS connector simply won‚Äôt fit in any incorrect way. Just put them into the holes on the board. You can stick them in place with some tape, but I like to just keep my finger on it while soldering on the other side. Flip the board and apply some solder. You can use some flux to make it easier.\n‚Äî\nüí£ How hard to fix: Not very hard, just use solder pump on all the pins and pull it off. TRRS is a bit more work as it has more pins.\n‚Äî\nSoldering OLED pins Temperature: 300‚Äì350 ‚ÑÉ\nDifficulty: easy\nHow hard to fix: medium\nIt‚Äôs basically the same as with the Pro Micro. Just make sure you‚Äôre doing it on the correct side of the PCB. Longer part of the ping should be located on the top side of the board. If you have the cover, it might be worth to just put the pins into the holes, screw spacers in and check if it fits. In my build spacers were 0.5mm short and I had to lower the pins a bit. You can just move the plastic part on the pins up and down by applying some force with small pliers.\nProgramming Pro Micros (optional) Temperature: None\nDifficulty: easy\nHow hard to fix: easy\nNow this step is a bit tricky, as we‚Äôre going to be shortening pins on the board manually. That‚Äôs why I marked it optional. You can do all this using reset button we have soldered into the main board, but it‚Äôs good to test our microcontrollers before soldering them in. Desoldering is a big pain here, we want to make sure they are working correctly. We‚Äôll start with default firmware, as it has all the features enabled and we‚Äôll be able to test our keyboard. You can flash anything you wan later on, but don‚Äôt use the ‚Äòcommon‚Äô firmware from QMK configurator for now. It has OLED support disabled and you might spend quite some time (as I did) trying to figure out why they don‚Äôt work.\nBut first a BIG WARNING:\n‚Äî\n‚ö†Ô∏è Pro Micros are sensitive to static current. Its this small shock you sometimes feel when touching things out of metal. It can destroy your microcontroller. So before taking it from the antistatic bag and each time before you touch it, touch something that‚Äôs out of metal and is grounded. Your room radiator will be best. Sometimes you‚Äôll feel the shock. That‚Äôs the one you saved your controller from.\n‚Äî\nNow grab the QMK toolbox here:\nDownload QMK toolbox\nWe will be using it to flash our controller. And base firmware here:\nDownload CRKBD firmware\n(If link doesn‚Äôt work, just search for ‚Äòcrkbd_rev1_common_via.hex‚Äô on this site)\nConnect your Pro Micro using Micro USB cable (be very gentle with it, this socket is very fragile). Start QMK toolbox app and:\nClick Open button and select downloaded ‚Äò.hex‚Äô file. Make sure you have ‚Äòatmega32u4‚Äô controller selected Use jumper wire or tweezers to short RST and GND pins on Pro Micro for a second. Make sure you don‚Äôt touch any other pieces of the board with your wire. If you do it right, QMK toolbox should show a yellow message about that it detected your controller. It should look similar to what you see below:\nDevice name might be different (depending on which Pro Micro clone you have), but there should be ‚Äúsomething connected‚Äù ü§™.\nNow press the Flash button and it should start baking firmware into your controller. The board will be in the programmable state only for few seconds, so if you get a ‚ÄúDevice disconnected‚Äù warning, just short the RST and GND again. After few seconds, if the flashing was successful, you should see something like on the screen below:\nNow your Pro Micro should show up as a regular keyboard (HID device).\n‚Äî\nü§î Advice: This part might be a bit tricky. You really have to watch out not to touch any other part of the board with your wire or tweezers while shortening the pins. If you don‚Äôt feel like you want to do this, just connect the Pro Micros and check if LEDs on them lightens up. Probably they are all fine and you will be able to flash them when soldered in.\n‚Äî\nSoldering Pro Micro controllers Temperature: 300‚Äì350 ‚ÑÉ\nDifficulty: easy\nHow hard to fix: VERY HARD\nNow insert your controllers onto the pins on the board facing down. The socket and all chips on the controller board should face the main PCB. You should see the empty bottom of the Pro Micro when it‚Äôs on the board. Looking from the side, a Micro USB socket should almost touch the main PCB:\nMake sure it‚Äôs in place and double check if it‚Äôs positioned correctly, desoldering it will be very painful. Apply some flux on both lanes of pins and solder each one of them, just like with the pins themselves.\n‚Äî\nüí£ How hard to fix: ‚ö†Ô∏è Watch out here. It‚Äôs really hard to desolder it if you don‚Äôt have a hot air station. You will spend quite some time fighting it, and then will have problems to fit new socket into the controller, as it‚Äôs hard to clean the holes from all the solder. So double check and then check it once more. To make sure you have everything placed correctly before soldering.\n‚Äî\nAs you can see in the photo above. I have longer pins on one side. That‚Äôs because I‚Äôve got it wrong and had to re-solder them. Didn‚Äôt have any more short pins and had to use a longer set. It was really pain in the a** to desolder it. Hope you‚Äôll have it right the first time.\nSoldering OLEDs Temperature: 300‚Äì350 ‚ÑÉ\nDifficulty: easy\nHow hard to fix: hard\nAfter you have your controllers in place it‚Äôs time to solder in the OLEDs. First place some insulating tape on your Pro Micro. It shouldn‚Äôt be needed, because theres nothing really conductive there, but better safe than sorry. Then put your OLED in place and check if everything is where it should be. And yes, you want display of the module facing up ü§™.\n‚Äî\nü§î Advice: If you have a OLED cover it might be a good idea to check now if it will fit. It will be easier to replace or shorten the pins now.\n‚Äî\nIf everything is in place press gently in the middle of the display with your finger and make sure it‚Äôs in place and positioned horizontally to the Pro Micro. It might be a bit hard to level it as it has some outstanding components at the bottom. Just do one pin first and check it it‚Äôs correct. Then apply some flux and solder rest of the pins.\nTest OLEDs and LEDs Temperature: None\nDifficulty: easy\nWe‚Äôre almost done, but now is a good moment to test what we already have. Theres a chance that some LEDs won‚Äôt be lightening up, and now it‚Äôs a bit easier to fix them than when we have everything in place.\nIf you have not flashed your controllers, now is the time to do this. Just follow the ‚ÄúProgramming Pro Micros (optional)‚Äù section and instead of shortening pins on the board press the reset button on the PCB. Remember, that you have to flash both halves of the keyboard.\nAfter flashing disconnect USB, connect both sides of the keyboard with TRRS cable (always do it before you connect your keyboard via USB, this cable has no protection and can short circuit the controller while being inserted or removed) and connect the USB cable. Your keyboard should lighten up. You should see a layer indicator and a keylogger on left half and a Corne logo on the right one. Also all the LEDs should be glowing in red.\nAs you can see. I have messed up some LEDs on the left half. Don‚Äôt worry, it‚Äôs easy to fix. If you also got something messed up, check the ‚ÄúTroubleshooting‚Äù section.\n‚Äî\nü§î Advice: Use a magnetic connector on the Pro Micro USB. Its socket is really fragile. It usually breaks, does it pretty soon and desoldering a Pro Micro is a pain in the a** as you already know.\n‚Äî\nCase and switches Temperature: None\nDifficulty: easy\nHow hard to fix: easy\nIf you have a case, now is the moment to assemble it. If your case is similar to mine, then first assembly the top part and connect all the switches. Test if they all work. If not‚Ää‚Äî‚Äächeck the ‚ÄúTroubleshooting‚Äù section.\n‚Äî\nü§î Advice: There are two ways to insert the switches. First is to insert 4 switches into corners of the case, put it into the board, screw the case and then insert others one by one. It usually works great, but with Corne it worked better for me to just insert all switches into the case and connect them all at once. YMMW, just try it out.\n‚Äî\nWhen you have all your switches in and your case assembled, just insert your keycaps. You‚Äôre done.\nThe end Now you should have a fully functional keyboard. If something doesn‚Äôt work check the ‚ÄúTroubleshooting‚Äù section at the end. I hope it was a fun journey. You can check out QMK firmware page to start customizing your layout or firmware.\nTroubleshooting Some LEDs are not glowing or are not red If one or more LEDs is totally dark, then probably you have an unsoldered leg on the first one that is not glowing or on the previous one. LEDs are connected in series‚Ää‚Äî‚Ääprevious one is powering next. So there are only 2 to check. The same is for LEDs that have different color than the rest. You only need to check faulty one or the previous. Unfortunately numbers on the PCB are not the same as the connection sequence. Check one of the pictures below from the Foostan guide to see the connection ordering:\nPer-key left part\nPer-key right part\nFor underglow you just need to guess or consult the schematics. But there‚Äôs only 6 of those, so it won‚Äôt be hard. If no under-key LED is working, check either the first one or last underglow LED.\nFirst check for soldering issues. Maybe one of connections looks bad. If you think that soldering is fine replace the LED. It might be overheated or faulty.\nNo LED is glowing This can be one of 4 problems. In order of probability:\nYou have disabled LEDs by using a keyboard shortcut. Check keymap for combination to turn them on again. You have flashed firmware without LED support. Check that you have RGBLIGHT_ENABLE = yes in rules.mk file for your keymap and check for define DISABLE_RGB_MATRIX_ALPHAS_MODS lines in config.h. Re-flash and check again. LED no. 1 is not soldered correctly or broken. Check soldering on LED number 1. If it‚Äôs all good, replace the LED. You have an unsoldered pin in your Pro Micro. Check all connections of Pro Micro. Maybe theres a pin that looks bad. One of the buttons is not working It can be one of following, check in following order:\nIt‚Äôs mapped to some modifier key in firmware. Check keymap. Switch is not fully in the socket. Check if it‚Äôs touching the PCB. One of the switch legs is bent. Remove the switch and straighten the leg. Diode on the switch is soldered incorrectly. Make sure it‚Äôs in the same direction as rest of the diodes. If not desolder the diode and solder it back correctly. Hot-swap socket is soldered badly. Re-solder. Switch is faulty, try another switch. Hotswap socket is either faulty or you have overheated it. Desolder and replace. Diode is faulty, desolder and replace. Whole row or column of the keys is not working You have a bad connection on the microcontroller. Check soldering on Pro Micro. Or you have messed up all the diodes on the row. Second one should be easy to spot.\nKeyboard is not recognized in the OS You have messed up firmware flashing or have a faulty Pro Micro. Start by reprogramming your controllers.\nMy keyboard halves are mixed If you‚Äôre pressing a button that should be on your left hand, but it is on the right and vice-versa, it means that you have connected wrong half to the USB. Unplug cable from the side it is in and plug it into the second half. You can also change firmware settings and re-flash if you want to keep cable in the current half.\n","permalink":"https://marekpiechut.github.io/post/2021-06-23_corne-mx-3.0-keyboard-build-guide/","summary":"Introduction ‚å®Ô∏è Typed with: Corne MX with Gateron Brown \u0026amp; BM40 with ultra-light Gateron Clear\nThis is a build guide for Corne Keyboard (Crkbd) Cherry MX keyboard version 3.0.1 by foostan. Original Japanese guide is available here. Unfortunately there‚Äôs no official guide in English for this particular version, but I‚Äôve been recently building one of them, so here it is. Please keep in mind that this version is not a translation and I don‚Äôt have even slightest Japanese knowledge.","title":"Corne MX 3.0 Keyboard build guide"},{"content":"Getting lazy loaded web page elements based on scroll position has been a performance or maintenance nightmare. You either loosen components encapsulation and build a complex mechanism with single scroll handler notifying elements when needed, or end up with huge amount of onScroll handlers that will kill page performance. Thanks to W3C working draft from 2019 being already implemented in all major browsers we can get rid of all this stuff. Just use IntersectionObserver and get fast and elegant lazy loads when needed.\nSo if you no longer care about IE compatibility (and please stop if you do, we need to kill that monster), then stay with me and check out how to cheaply get onScroll lazy loads in React. We‚Äôll base our example on exercise library we‚Äôve added to CrossKeeper - product we are working on.\nThe Problem Problem with this UI is that it can get slow pretty fast if we simply loaded all exercises with images and dumped them into the page at once. It‚Äôs also worth noting, that in this example we don‚Äôt exactly know which thumbnails to show based on just exercises. We have to query media source (which can be our custom media server or YouTube API) to fetch it. The only way to do it is one by one, no batch loading. So you might already guessed, firing few hundred HTTP calls each time user opens it up might feel slow. Caching helps a bit, but first load will be slow anyway and keeping all this stuff around if someone never scrolls to the bottom is also not most elegant.\nOn the image below you can see how it looks like performance wise when we go with render-all-at-once solution.\nWe have more than 7 seconds of network calls to fetch exercise media and thumbnails. Also there‚Äôs only so many connections you can initiate to each server. Depleting connection pool makes jumping to some other section of the app feel slow. We‚Äôre not allowed to fetch it‚Äôs data while waiting for media sources.\nUsing IntersectionObserver Let‚Äôs try to cheaply fix it using IntersectionObserver so we only fetch metadata and images for stuff we really need.\nFirst we need some UI we‚Äôll be rendering and store URL somewhere and make sure it‚Äôs rendered when ready. Div with background image and useState hook should fit in nicely:\nWe‚Äôll keep ExerciseItem self contained and make itself load required data from media sources, so we add effect to fetch data and do it only on mount and when media identifier might change:\nWhat we have now is fully functional UI component that will give us exactly performance we see in our problem description. All exercises will start fetching media metadata and images as soon as they are added to DOM. This is not what we‚Äôre expecting.\nLet‚Äôs fix it by adding IntersectionObserver and connecting it to DOM element via ref:\nThis should work pretty nicely. The only thing that‚Äôs awkward is that images start to load when user does already see them, giving him impression that page does not keep up with his scrolling speed.\nFortunately there‚Äôs something baked into IntersectionObserver that can help us out. If you check API for it, you‚Äôll notice we can pass some options to the constructor (Intersection observer options) and rootMargin is what we need.\nLet‚Äôs make sure that we start loading stuff 200px ahead of user scroll (format is like in CSS margins value). This should cover it up nicely:\nFor final touches we might add graceful fallback to previous solution on old browsers and here‚Äôs what we have:\nCleanup It doesn‚Äôt look that bad, but let‚Äôs give it a bit of refactoring, so we‚Äôre not overwhelmed with details each time we open up this component.\nResult Well, there you have it. Fully functional and reusable hook to handle lazy loading using IntersectionObserver. Of course you can easily add loading and error indicator or generalize it even further to make it reusable (ex: passing root margin and loader function from the outside). But even like that it\u0026quot;s a huge improvement over original.\nBelow you can see how it looks like to open exercise library now. We‚Äôre able to show fully functional UI with all images nearby in 1 second‚Ää‚Äî‚Ääsame machine, same server and same network.\nAll this with just few lines of code and modern browser. Great stuff going on in the web technologies and let‚Äôs hope that, thanks to browsers auto-update, we‚Äôll be able to use more and more of these in our day to day work.\n","permalink":"https://marekpiechut.github.io/post/2020-07-07_get-lazy-loading-cheap-with-intersectionobserver-in-react/","summary":"Getting lazy loaded web page elements based on scroll position has been a performance or maintenance nightmare. You either loosen components encapsulation and build a complex mechanism with single scroll handler notifying elements when needed, or end up with huge amount of onScroll handlers that will kill page performance. Thanks to W3C working draft from 2019 being already implemented in all major browsers we can get rid of all this stuff.","title":"Get lazy loading cheap with IntersectionObserver in React"},{"content":" Most of GraphQL APIs that are developed are probably not meant for public access without any authorization. Sooner or later you‚Äôll need to somehow limit access to only authenticated users or limit resources so that only allowed users are able to see them.\nIn this post, we‚Äôll take a look at how you could implement GraphQL security in applications using NodeJS, Passport and Apollo Server.\nWays to implement authentication There are a few ways you could add access rights to your GraphQL APIs:\nIf your requirements are simple, you can just allow all access to logged in users and decline it to the general public. This could be easily done in a context factory (we‚Äôll go through that later on). You could check access rights in your resolvers and fine grain it based on requested data, current session and returned objects. Move security code into your domain logic and fail there if access should not be granted. Add directives to your schema and make security declarative. Leave all security to other services if your GraphQL is only a facade to other systems. Where to keep security code I think, that security is rarely something domain specific but rather part of system infrastructure and does not belong to domain code. Moving it to domain layer would also mean you make it dependent on session data and web application concepts. That‚Äôs not what we usually want.\nKeeping it in directives may look very elegant at first, but it makes things really complex if you need something more than simple role-based authorization. You might end up with a brand new DSL on top of GraphQL schema language to get something more from it.\nWhat we have left is to keep it in GraphQL resolvers and that‚Äôs what we‚Äôll focus on in this article. Keeping authorization code there is very elastic and does not pollute your domain logic.\nStarting Point Technologies we‚Äôll be using:\nNodeJS ExpressJS Passport Apollo Server 2.0 GraphQL Playground To make things simpler I assume you already have Passport configured and some basic way to authenticate in your system. If you need help with that, check out this article on RealLifeProgramming to get you up and running.\nI also assume you have some knowledge about GraphQL and some GraphQL API that we‚Äôre going to secure. Make also sure you have GraphQL Playground or some other way to test things out.\nKnow your user To do any access right checks we first need to know who is trying to access the resources. When using Passport we already have everything in place and can retrieve the user from the session (or get nothing if the user is not authenticated).\nTo retrieve Passport session data we need to first make sure to setup Passport before registering Apollo middleware, so when we hit any resolver we‚Äôre already authenticated. We just need to make sure our code is in right order:\nWith this code in place, our GraphQL API is still publicly available. What we can do now, is pass logged in user (if available) down to all resolvers using Apollo context.\nIf all or nothing authorization scheme is what you need, then you can also authorize all requests here. What you can do, is simply throw AuthenticationError from apollo-server-express package whenever there\u0026quot;s no user, or your user is not allowed to access graph:\nAlthough this might solve your current problems I don‚Äôt really recommend it. It‚Äôs not very elastic and is hard to extend later on. It also forbids any public APIs you might want to add in the future, so it might be better to move to the second step and add some security logic directly to graph resolvers.\nTesting things out But first, let‚Äôs check if everything works as expected. We can use GraphQL Playground to test things out. You should see something similar when trying to query without login:\nGraphQL Playground‚Ää‚Äî‚ÄäAuth Error\nJust one small note before we continue: before you try to access secured resource make sure you have request.credentials option set to at least same-origin in playground options. Without it, there will be no authentication data sent to your backend when you\u0026quot;ll be executing your queries.\nSecuring resolvers Let‚Äôs start with moving our all or nothing code into resolvers, so we can choose on per resolver basis if we allow access or not. What we need to check is a user property in context object (3rd argument to resolver):\nOk, this solves our problem, but copying all this stuff around would be very cumbersome. Why not take advantage of that Javascript is a functional language and we can pass functions around. Let‚Äôs refactor this code to a higher order function that we can later apply on any resolver we like:\nNow we only need to wrap our resolver with requiresLogin call:\nRole-based authentication By changing only a few lines here, we could also get a very nice role-based authentication using this pattern:\nAnd use it in similar manner:\nModularized resolvers All this can get you far, but will probably also get cumbersome after some time. When you start to divide your resolvers, you‚Äôll end up wanting to setup access rights for whole groups in one place instead of wrapping functions one by one. Again we would end up with a lot of copied code.\nWhy not simply make our authorization function support whole objects and wrap all resolvers with auth code:\n*isFunction, isObject and mapValues functions come from LoDash\nNow we can wrap our resolvers with requiresRole on any level and it will recursively secure everything according to the declaration:\nIn this example, both: templates and tasks queries will require the current user to be a \u0026ldquo;member\u0026rdquo; before he can access them.\nP.S. If you don‚Äôt modularize your resolvers yet, check out Modularizing your GraphQL schema code by @dpandya.\nWhat next Keeping authorization logic in resolvers is very elastic and we‚Äôve only touched surface here. Thanks to that resolvers are asynchronous in nature and can return Promises we can implement pretty complex authorization schemes with pre and post validation and even custom per-entity ACLs. But we‚Äôll cover these in another article.\n","permalink":"https://marekpiechut.github.io/post/2019-01-21_authentication-and-authorization-in-nodejs-graphql-api/","summary":"Most of GraphQL APIs that are developed are probably not meant for public access without any authorization. Sooner or later you‚Äôll need to somehow limit access to only authenticated users or limit resources so that only allowed users are able to see them.\nIn this post, we‚Äôll take a look at how you could implement GraphQL security in applications using NodeJS, Passport and Apollo Server.\nWays to implement authentication There are a few ways you could add access rights to your GraphQL APIs:","title":"Authentication and Authorization in NodeJS GraphQL API"},{"content":"Who wouldn‚Äôt want to get something for free? If you‚Äôre using Webpack with React you can get size and load time optimizations with just a few small configuration changes. Almost for free, without messing with your code.\nHere I‚Äôm showing few of these. They are really easy to do and can give you nice results regarding application size and startup time.\nSwitch Node to production When switched to production, Webpack will add optimizations not executed otherwise. What‚Äôs even more important‚Ää‚Äî‚ÄäUglifyJS will remove all React debugging and introspection code. As it becomes effectively a dead code. It was helping you to find errors in development, but only causes slowdown and bundle bloat in production. Stuff like invariants checking, props validation, etc. There‚Äôs a lot of code like this in React:\nand it will be completely removed in production mode. Thanks to our little environment flag.\nAdd this code to production plugins in webpack.conf.js to enable it:\nand remember to launch webpack with -p and --env.production options: webpack --env.production -p\nStrip all unneeded stuff with UglifyJS Your production build probably includes a lot of stuff that‚Äôs not really required during execution. With some more aggressive UglifyJS configuration you can also get rid of these:\nIt will wipe a lot of stuff you don‚Äôt really need. Some of it might be useful during development but is only a waste when you release. I personally like screw_ie8 the most. It‚Äôs just something we all should have done quite some time ago. This option should be enabled by default.\nLoad only required Lo-Dash modules If you‚Äôre building a larger application, chances are, you‚Äôre already using Lo-Dash. You can get some bundle size reduction by using babel-plugin-lodash to enable tree shaking for Lo-Dash imports. It analyzes your code, to check which functions are imported and which can be skipped. What it means, is that you‚Äôll only include stuff you are really using (and it‚Äôs dependencies), not everything that‚Äôs in the package. And it doesn‚Äôt matter if you‚Äôre using import map from \u0026quot;lodash/map\u0026quot; or import {map} from \u0026quot;lodash\u0026quot; notation. It‚Äôs smart enough to figure it out.\nGet rid of unused moment locales Moment is a great library, that has a huge set of locale-specific settings, that you might actually never use.\nIt might be useful to get rid of ones you don‚Äôt use. If you have support for only subset of languages or are building an internal app and you need no internalization at all, there‚Äôs no point in keeping them around.\nHere‚Äôs the difference it can make (file sizes before minification, so final results might vary):\nWith all locales: moment: 455.55 KB (35.9%) With only en locale: moment: 125.92 KB (13.4%) With en and pl locales: moment: 129.59 KB (13.8%) As you can see, leaving only a handful of these can save a bit of space. Here‚Äôs how you set it up:\nFirst, filter out all locales using webpack-ignore-plugin (it‚Äôs built-in, so nothing to install):new webpack.IgnorePlugin(/^\\.\\/locale$/, /moment$/),\nThen, in case you need something more than plain en, require it explicitly. You can do it by adding something like this somewhere in your code (probably near app initialization, where you setup moment stuff):import \u0026quot;moment/locale/pl\u0026quot;\nWebpack will make sure it‚Äôs included in the output, even if it was ignored before.\nSplit bundle to vendor and app code If you have more than one entry-point and there are parts of it that some users won‚Äôt ever access (like admin panel). Or you have more than one app in a single codebase (like variants of the app for different departments). Then it might be useful to split distribution package to separate bundles and keep external dependencies apart. This way users that don‚Äôt need admin module won‚Äôt download and parse its code. And your dependencies will be in a separate file, that browser can cache and reuse when switching between apps.\nYou should probably also have your entry-points declared somewhere in configuration object:\nJust make sure you include vendor script before your application in entrypoint html file.\nThere‚Äôs also a chance, that after some time, your dependencies stop changing so often and you might release updates without even changing vendor bundle. This way clients won‚Äôt need to fetch it again.\nOf course you can experiment with CommonsChunkPlugin to get even better results. Maybe you‚Äôll want to keep separate bundle with dependencies used in each variant of the app or extract only stuff that‚Äôs really shared between entry-points. Check out docs here: commons-chunk-plugin to see what else you can get from it.\nBonus‚Ää‚Äî‚Äähow to analyze bundle size So now you have set up some optimizations that should lower your bundle size and decrease startup time. But what more can you do to make it smaller and faster? Get rid of stuff that‚Äôs taking a lot of space and doesn‚Äôt give much value. Sometimes you include library and use only a tiny fraction of it just to find out that it‚Äôs a half of your application size.\nTo know which libraries consume most space, we need to look into Webpack bundle and analyze what‚Äôs there. There are two nice tools to get some insight:\nWebpack bundle analyzer and\nWebpack bundle size analyzer First of them gives you nice visual map to click through and check what takes up most of the space and what it depends on.\nIt‚Äôs very good if what you‚Äôre doing is a hunt for space hogs. I highly recommend you run it at least once on your project. You might be surprised. It‚Äôs not always obvious what is the biggest part of your app.\nOn the other hand, if you only want to check what‚Äôs the impact of stuff you just added or quickly get some insight Webpack bundle size analyzer might be the way to go. You just pipe Webpack JSON output to it and you‚Äôre done: webpack -p --json | webpack-bundle-size-analyzer | less\nand here‚Äôs the output:\nSet the options, but what are the results? Just to sum things up. Here are bundle sizes for DOS React starter kit with:\nVersion: webpack 3.6.0 Time: 13194ms Asset Size Chunks Chunk Names main.js 7.29 kB 0 [emitted] main vendor.js 390 kB 1 [emitted] [big] vendor styles.css 2.25 kB 0, 1 [emitted] main, vendor index.html 463 bytes [emitted] and without configuration changes applied:\nVersion: webpack 3.6.0 Time: 10899ms Asset Size Chunks Chunk Names main.js 571 kB 0 [emitted] [big] main styles.css 5.57 kB 0 [emitted] main index.html 463 bytes [emitted] As you can see, we have stripped almost 200kB from final size without any changes in source code or removing any libraries. It was probably worth these few minutes to put it in the config files.\nBy the way, if you want a React boilerplate to speed up your next project ramp-up, give DOS a try. It has all these optimizations already configured and ready to be used.\nAnd here is a full webpack config file for reference: webpack.config.js\n","permalink":"https://marekpiechut.github.io/post/2017-11-02_get-rid-of-some-fat-from-your-react-production-build/","summary":"Who wouldn‚Äôt want to get something for free? If you‚Äôre using Webpack with React you can get size and load time optimizations with just a few small configuration changes. Almost for free, without messing with your code.\nHere I‚Äôm showing few of these. They are really easy to do and can give you nice results regarding application size and startup time.\nSwitch Node to production When switched to production, Webpack will add optimizations not executed otherwise.","title":"Get rid of some fat from your React production build"},{"content":"‚Ä¶ or does it make sense to use new or Object.create anymore? We all love and use factories and object literals in our JS code. It‚Äôs clean, readable, there‚Äôs little space for errors and (the best of all) it allows private state. (Check here if private members in Javascript is something new to you.)\nBut there‚Äôs something worrying about them\u0026hellip; Check out the following code:\nIt looks like we‚Äôre creating new add , sub and getVal functions over and over again. Just so they can have a separate closure scope with separate private variables (some more about this here).\nHey, but maybe Chrome‚Äôs virtual machine can optimize this stuff?\nCan Chrome optimize it? ‚Ä¶ and what is a real performance impact of using this technique? Is it really a performance problem or is it a marginal issue? Let‚Äôs check it out.\nI‚Äôve prepared a quick benchmark to test it. What I was looking for is a memory footprint differences between good ol‚Äô prototypes with new, Object.create and closure with object literal.\nChrome has pretty nifty dev-tools with memory profiler and a quite well optimized virtual machine. So we‚Äôll be using that. If they don‚Äôt have it optimized, then probably no other browser has.\nUsing prototypes Let‚Äôs start with benchmark code. It‚Äôs pretty simple and looks more or less like this:\n‚Ä¶full code here\nIt creates 1,000,000 objects with few fields and methods so we can get a bit more realistic results. Call tosetInterval and method calls are there so the browser has no chance to remove it due to ‚ÄòDead code elimination‚Äô.\nOf course, all other optimizations are allowed. That‚Äôs exactly what we want to find out. If the browser has somehow optimized this stuff, so we don‚Äôt have to.\nLet‚Äôs run the benchmark, take memory snapshot and looks what‚Äôs there:\nAs you can see we have used 58MB of memory for our dummy objects. And 10 MB of it is used by the array just to keep references to the instances. We shouldn‚Äôt really count this in. All of methods are inside shared prototype object (it has the same memory address: __proto__ = @2121167). This result will be our starting point. Nothing is wasted, all functions are reused (‚Ä¶and there‚Äôs no private state).\nUsing closure and object literal This is probably the most elegant solution. It‚Äôs very readable, allows for private state and is nicely hidden from the outside. But what‚Äôs the memory footprint?\nNow, the heap size is huge. We‚Äôre using 490MB of memory for the same 1,000,000 objects.\nAnd here are file sizes of the heap dumps: 107M Proto-1000000.heapsnapshot 800M Closure-1000000.heapsnapshot\nThis alone shows how huge is the difference.\nIf you check the profiler output you‚Äôll see, that we have a separate instance for each method with a copy of the same code. Every add , getVal , etc. has a distinct memory address. Nothing is optimized here (and I really doubt you can safely do it, to be honest).\nNow, for the completeness, let‚Äôs check how Object.create behaves.\nUsing Object.create More modern than new and less elegant than closures. We‚Äôre using common base object with all methods (almost like with new) and a small factory using customizer to add instance variables:\nWe‚Äôll declare all variables as writable, just to be fair with prototypes. And here are the results:\nIt‚Äôs 66MB. Not bad. Almost the same as the new operator. We still reuse all functions code, thanks to the same __proto__ instance and don‚Äôt get any unneeded duplicates.\nThe verdict Giving a clear recommendation to use one technique over the other is difficult here.\nThere‚Äôs definitely a memory footprint penalty for using closures and object literals. But in 99% of cases you won‚Äôt be instantiating so many of them that it becomes a problem. Also GC should usually clean it up, when they are no longer needed.\nWhat you get though is nice, readable code and total privacy, unless you make something public.\nOn the other side. If what you really need is raw performance, maybe hiding new or Object.create inside some factory is a way to go. But as always ‚ÄúPremature optimization is the root of all evil‚Äù, so maybe it‚Äôs not worth it.\nThere‚Äôs also a kind of a middle ground here: use prototypes and hide all private members by declaring them inside constructor (check Privileged section in Douglas Crockford‚Äôs article).\nSo here it is. As you see having this and prototypes can still be useful. At least can save us some memory so we can use it for something more important than copies of same functions.\n","permalink":"https://marekpiechut.github.io/post/2017-08-22_will-chrome-optimize-your-object-factories/","summary":"‚Ä¶ or does it make sense to use new or Object.create anymore? We all love and use factories and object literals in our JS code. It‚Äôs clean, readable, there‚Äôs little space for errors and (the best of all) it allows private state. (Check here if private members in Javascript is something new to you.)\nBut there‚Äôs something worrying about them\u0026hellip; Check out the following code:\nIt looks like we‚Äôre creating new add , sub and getVal functions over and over again.","title":"Will Chrome optimize your object factories"},{"content":"This post was previously posted on our project blog and here is a slightly updated version. Right now QuotesKeeper is already in AppStore and we‚Äôll be posting updated thoughts about React Native soon.\nSome time ago at DayOne.pl we started a project to build 3 apps with 3 devs in 30 days‚Ää‚Äî‚Ääthe one month project. One of our mobile apps was built using React Native. Here are my thoughts after more than 2 weeks of development (while the thing was still in the making).\nP.s. Ok, it took a little bit more than 30 days. We had 2 weeks delay due to sync functionality we had to do in Obj-C. But more about it in next post.\nExtremely easy to start React Native is really easy and straightforward to start with. You can set the project up and start coding in about 5 minutes. Everything is taken care of‚Ää‚Äî‚ÄäXCode project setup, JS environment, templates, simulator, etc. You just run built-in project generator and most of the stuff is done for you.\nSecond‚Ää‚Äî‚Ääif you have some React experience you‚Äôll feel right at home. Everything is familiar and works almost the same way. Most of the knowledge you have about components, lifecycle, state, props, mixins, etc. can be used in your React Native project. You can use Flux or Redux to manage application state and EventEmitter to notify UI about changes. Just like you would do in regular React app.\nThere are plenty of libraries ready to be used by your project if any native component is missing. Also calling native code is super-easy. It‚Äôs so easy, that in many cases it was faster for me to write simple Objective-C function and call it from JS instead of searching for some library that had it ready.\nDeveloper happiness React Native has one absolute killer feature‚Ää‚Äî‚Ääcode-test-code cycle. It is the best programming environment you can get when building mobile UIs. Period. You can open iPhone simulator next to your code editor and see changes constantly being applied while you edit files. It‚Äôs really a pleasure to work with. Every time I need to change native code and re-run the whole app I feel like I‚Äôm traveling back to ‚Äô90s. Even if you don‚Äôt have ‚Äúhot reload‚Äù enabled and need to hit Cmd+R sometimes, it‚Äôs still freakin‚Äô great! Yes, it really works like in this GIF on React Native homepage.\nAll the other nice things There are more nice things about it. Performance is great when compared to ‚ÄúHybrid‚Äù solutions. Building UI that‚Äôs compatible with iPad and iPhone is easy. Styling with flex is really nice and easy. Still, there are some‚Ä¶\nParts we didn‚Äôt like React Native is still young and you can see it in many places. Some properties are missing, even in core components. Documentation is lacking and you often have to dig into framework code to see how something works. Many external libraries still need much work and have nasty bugs. Of course, this is something to be expected from open source project in this early phase, but you need to take it into consideration if your project is large and critical for your company.\nBut the biggest issue we‚Äôve had is with the performance. Wait! What? Didn‚Äôt I say that performance was great? Yes, I did. Don‚Äôt get me wrong. The app is very responsive and probably you won‚Äôt have any issues if you‚Äôre not displaying very long lists or processing large amounts of data. But if you do, you‚Äôre in trouble. JS performance is nowhere near Objective-C code.\nWe started having performance issues with around 2000 objects in the list that we needed to process end filter. Moving it to native code fixed everything. Also built-in React Native list component start scrolling very slow when you have so many elements. You really need to test things out if you plan to work on large datasets.\nSummary To sum things up‚Ää‚Äî‚Ääwould I use React Native in my next project? The answer is‚Ää‚Äî‚Ääprobably yes. It‚Äôs really good and fast. Developer experience is great and what you‚Äôre building is a real app, with native look and feel. You‚Äôll have a chance to test it yourselves in Quotes Keeper.\n","permalink":"https://marekpiechut.github.io/post/2017-02-13_react-nativefirst-impressions/","summary":"This post was previously posted on our project blog and here is a slightly updated version. Right now QuotesKeeper is already in AppStore and we‚Äôll be posting updated thoughts about React Native soon.\nSome time ago at DayOne.pl we started a project to build 3 apps with 3 devs in 30 days‚Ää‚Äî‚Ääthe one month project. One of our mobile apps was built using React Native. Here are my thoughts after more than 2 weeks of development (while the thing was still in the making).","title":"React Native‚Ää‚Äî‚Ääfirst impressions"},{"content":"I‚Äôve been working with Git for some time now, probably 5 years or so. Tried (or been forced to) quite a few other VCS-es before (CVS, SVN, Mercurial and even some Perforce). To be honest I wouldn‚Äôt like going back to any of them. Git is so good and gives you so fine grained control over your repository you won‚Äôt miss your old VCS. It lets you work in a workflow that‚Äôs ideally suited for you, yet allows keeping everything clean and predictable in public/release code.\nOn the other side, I see a lot of people struggling with it at the beginning and desperately looking for some advice how to structure their repository. What they usually find is GitFlow process described by Vincent Driessen in his post: A successful Git branching model. While at first it looks really nice, it has few misconceptions about how Git works and makes your work a bit painful after your projects grow.\nIn this article I want to show you a simpler process that works really great with small teams and linear development, but also scales to big teams with simultaneous releases you need to support at the same time.\nEverything goes to master, develop is a waste Every new feature and bugfix should end up in master. There‚Äôs no need in Git to have a branch if you want to only tag commits. You can tag any commit and it doesn‚Äôt have to be on any particular branch. So in GitFlow ‚Äúnomenclature‚Äù you could use only develop branch and remove master altogether, but there‚Äôs one issue with that. Everybody in the world expects to see latest changes in master after they clone your repo. Every tool out there considers master as the current development branch. So just delete this develop nonsense and make master tip source of your latest and greatest features.\nNow, you don‚Äôt want master to be full of interleaving commits, so you do all your work in‚Ä¶\nShort living feature branches Each time you start a new feature it should begin with a new branch. Naming is not important, they won‚Äôt last long. Probably a day or two.\nThe way I work with these is that I create a lot of commits with half-baked code and informal messages. I really create a lot of mess. Sometimes just to switch to another branch and check things out or to help someone out with his code. But when a feature is ready I do a careful rebase over origin/master and squash all this stuff into one or two meaningful commits that clearly represent the feature.\nOf course, you can introduce some more formalized process, especially when more people work on feature (branch). Still remember, that local commits are for free. Nobody sees anything until you run git push and you can always clean up the mess before pushing.\nAll these feature branches end up in master via merge (of course after rebase to keep history clean). Do it as often as possible. The more often you integrate, the better.\nOh, and after merge (with fast-forward) you should delete feature branch. It‚Äôs worthless to keep them around if you made commit messages meaningful and track features/bugs in a bug tracker. They just add noise when browsing history.\nReleasing All you have to do when releasing a new version is to create a tag on master. Mark commit you consider ready and that‚Äôs it. git tag v1.0.0 is really enough. You don‚Äôt need any release branch yet. Just create tag, check out and build release package.\nAfter release, you simply go back to master and work as usual on your new features.\nRelease bugfixing So far everything was very straightforward, but what to do if you need to fix a bug in release while you already have new features on master?\nSimple. Just create new branch release-branch-1.0 on commit tagged with v1.0.0. Check it out and do your bugfixes, just like you would do it on master. When everything is ready you create new tag v1.0.1 and release.\nRemember that release branch is, like master, always in a releasable state. Things not ready for production should not land here or should be disabled with feature toggle.\nBack(Forward)porting bugfixes After fixing a bug you‚Äôll of course want to incorporate it also on other supported versions that apply. There are 2 ways you can do it‚Ää‚Äî‚Ääby merging or by cherry-pick.\nWhich one should you choose depends on how many versions you need to support and where the bug was initially discovered and fixed. If the bug was fixed on release and you only need to support master‚Ää‚Äî‚Ääno problem, just merge release branch back to master and do it after each bugfix. Things get more complicated when you need to push it to more places or bug was fixed on master and you want to backport it to older versions. Then you need to cherry-pick changes to all relevant branches.\nI would use cherry-pick anyway, just to be consistent.\nMany simultaneous releases Now what‚Äôs really good about this model is that it scales to really big projects. We‚Äôve been using it on projects with 3 simultaneous production releases and some more in pre-prod testing. The project was really large with more than 100 developers working on the codebase. Yet this GIT process worked pretty well. We just had many open release branches which were receiving bugfixes and some small improvements. Some of them were ported to future releases, some were backported to older ones. Everything worked nicely.\nSummary I‚Äôm sure there are some weak points in this model and it might not fit well in your case. Though it worked very well for us on small and large projects with a centralized master repository. So if you‚Äôre still not sure how to structure your next GIT repository just give it a try.\n","permalink":"https://marekpiechut.github.io/post/2017-02-08_git-process-that-works-say-no-to-gitflow/","summary":"I‚Äôve been working with Git for some time now, probably 5 years or so. Tried (or been forced to) quite a few other VCS-es before (CVS, SVN, Mercurial and even some Perforce). To be honest I wouldn‚Äôt like going back to any of them. Git is so good and gives you so fine grained control over your repository you won‚Äôt miss your old VCS. It lets you work in a workflow that‚Äôs ideally suited for you, yet allows keeping everything clean and predictable in public/release code.","title":"Git process that works - say no to GitFlow"},{"content":"It gets very annoying when web app keeps hanging on server requests, only to fail with a timeout few seconds later. Or uses up all available network connections (remember, we only have handful of these) just to wait for failure. Especially if feature is non critical and it does many updates while you are interacting with the app. Wouldn\u0026rsquo;t it be nicer if, after failing, app simply stopped trying for some time? Giving backend service a chance to get healthy again. Or just to stop pissing off the user.\nCircuit breaker is what you need to get exactly that. What\u0026rsquo;s even better you need just a single one that can wrap all asynchronous calls in your app.\nWhat is a circuit breaker Circuit breakers are common in electrical engineering (Wikipedia). They simply interrupt the circuit after detecting a fault in current flow, so there\u0026rsquo;s no risk of fire, damage or injury. When everything is fine again you can re-enable (close) a circuit breaker and it will let the current flow. This is very similar to what we want from our backend calls - to stop calling broken backend after failure and retry when there\u0026rsquo;s a chance it was fixed.\nConcept of circuit breakers in software was popularised by Michael T. Nygard\u0026rsquo;s in his book: \u0026ldquo;Release It!\u0026rdquo;. (Which is a very good read by the way. You should check it out if you haven\u0026rsquo;t already).\nBasically it\u0026rsquo;s all about wrapping code that can fail with decorator, that is forwarding all calls and waiting for failure. Shutting down whole thing if it\u0026rsquo;s failing too often.\nCircuit breaker can be in one of 3 states: CLOSED, OPEN and HALF OPEN. When everything is fine, we just forward calls, as there would be nothing between caller and wrapped code (it\u0026rsquo;s the \u0026ldquo;CLOSED\u0026rdquo; state). Each failure is recorded and, when certain threshold is met, we disable call forwarding and start to fail immediately (now we\u0026rsquo;re in \u0026ldquo;OPEN\u0026rdquo; state).\nThere\u0026rsquo;s also this third state - \u0026ldquo;HALF_OPEN\u0026rdquo;. It\u0026rsquo;s little bit more complicated:\nWhile throwing errors, without even hitting backend, we do a check if grace period has passed. If it did, we try again. This time only once (now we\u0026rsquo;re in this \u0026ldquo;HALF OPEN\u0026rdquo; state). If everything went well we go back to normal (\u0026ldquo;CLOSED\u0026rdquo;) state or to failure (\u0026ldquo;OPEN\u0026rdquo;) if failed. Only difference is that we don\u0026rsquo;t check for threshold. First failure in \u0026ldquo;HALF OPEN\u0026rdquo; makes all go down again. No retries.\nHere\u0026rsquo;s a graph to illustrate all the state changes:\n![State diagram]({{ site.url }}/img/posts/circuit-breaker-diag1.png){: .img-fluid .full-page}\nImplementing circuit breaker in JavaScript Here\u0026rsquo;s a simple implementation of circuit breaker in JavaScript working with Promise returning calls:\nconst CLOSED = Symbol(\u0026#39;CLOSED\u0026#39;) const OPEN = Symbol(\u0026#39;OPEN\u0026#39;) const HALF_OPEN = Symbol(\u0026#39;HALF_OPEN\u0026#39;) const MSG = \u0026#39;Functionality disabled due to previous errors.\u0026#39; module.exports = (asyncFn, gracePeriodMs = 3000, threshold = 1, message = MSG) =\u0026gt; { let state = CLOSED let failures = 0 let openedAt function handleSuccess(value) { if(state !== CLOSED) { state = CLOSED failures = 0 } return value } function handleFailure(error) { if(state === HALF_OPEN || state === CLOSED) { failures += 1 if(failures \u0026gt;= threshold) { state = OPEN openedAt = Date.now() } } throw error } function tryReset() { if(state === OPEN \u0026amp;\u0026amp; openedAt \u0026amp;\u0026amp; Date.now() - openedAt \u0026gt; gracePeriodMs) { state = HALF_OPEN return true } } return function () { if(state === CLOSED || state === HALF_OPEN || tryReset()) { return asyncFn.apply(asyncFn.this, arguments).then(handleSuccess, handleFailure) } else { return Promise.reject(new Error(message)) } } } How to use it? Simply wrap function returning a Promise with circuit breaker and use it normally:\nfunction callBackend(url) { return fetch(url) .then(parseResponse) } const withCircuitBreaker = circuitBreaker(callBackend) withCircuitBreaker(\u0026#39;http://my.backend.com\u0026#39;) .then(doSomethingWithData, handleError) It\u0026rsquo;s a quick implementation and will only work with promises, but still can probably cover most of your needs. It\u0026rsquo;s because circuit breakers are so simple.\np.s. you can use code from this post however you like, just copy it to your project if you need. No attribution or anything required.\nI\u0026rsquo;ve also published it on npm as simple-circuit-breaker, so you can simply require it.\nTo sum it up Circuit breakers are really nice and simple pattern you can use to make your application more user and resource friendly. Especially when having some non critical functionalities that can be disabled for few seconds and nobody might even notice. Consider using it in your app integration points. It costs almost nothing and makes your user experience much more pleasant.\n","permalink":"https://marekpiechut.github.io/post/2016-09-16-circuit-breakers/","summary":"It gets very annoying when web app keeps hanging on server requests, only to fail with a timeout few seconds later. Or uses up all available network connections (remember, we only have handful of these) just to wait for failure. Especially if feature is non critical and it does many updates while you are interacting with the app. Wouldn\u0026rsquo;t it be nicer if, after failing, app simply stopped trying for some time?","title":"Circuit breakers - You don't need to wait for failures."},{"content":"It\u0026rsquo;s almost a year now since I\u0026rsquo;m working with React and Flux. After all this time I think I\u0026rsquo;ve learned a lot about how to build React apps, so they are easy to test, maintain and that components can be easily reused.\nHere are some of my most important conclusions. I think following them will make your life really easier, especially if you\u0026rsquo;re just starting with it.\nIs it worth it? Everybody at my team is still happy with react. Our app is quite big now and, despite having some issues integrating with external libraries, it\u0026rsquo;s still great experience. Of course we had to refactor our code and change working patterns quite a few times during the project. Mostly because React was pretty new when we started and proven \u0026lsquo;best practices\u0026rsquo; were yet to be seen. Yet it\u0026rsquo;s still a nice experience to work with it.\nCompared to other technologies we have used, React is really fast to start with and has very few concepts to grasp to be productive. It also works very well with immutable data and makes it easy to reuse components.\nIf you\u0026rsquo;re still unsure if React is for you, I highly recommend you give it a try. It\u0026rsquo;s really good and you can be pretty productive even after your project has grown big.\nProject structure From the beginning we started to organize our code after domain concepts and functionalities. This is simply the best way to keep stuff around. Forget all this nonsense about actions, components, stores folders. After your project grows it\u0026rsquo;ll make you suffer. Keeping all business related stuff together makes moving things around really easy. You can easily find things you plan to change, remove functionalities and refactor.\nSo how does our directory structure looks like? Here\u0026rsquo;s an example:\nROOT |- src |-- core (application core abstractions and main component) |-- components (basic reusable components that are not domain specific, ex: confirm popup, section, date selector, etc. |-- modules (here goes all the important stuff) |--- advanced-search |---- advanced-search-component.jsx |---- advanced-search-component.test.jsx |---- advanced-search-store.js |---- advanced-search-store.test.js |---- advanced-search-toolbar-component.jsx |---- advanced-search-actions.js |---- index.js (what we export in this module) |--- user-profile |---- ... |- test |-- pages (Page objects for e2e tests) |-- e2e (Selenium end to end/UI tests) As you can see, we keep unit tests in same directories as production code. This way we can have relative import paths and we don\u0026rsquo;t have to change anything when we move whole modules around. Want to move some components to another module? No problem, just move all files.\nIt\u0026rsquo;s also not a problem when we release. Tests are filtered out from production bundle with simple regular expression.\nIt\u0026rsquo;s also a good idea to keep index.js file inside each module and make sure external code use only module path to import stuff. It allows you to make import names shorter and more meaningful. It\u0026rsquo;s also a form of \u0026lsquo;poor mans\u0026rsquo; encapsulation:\nconst AdvancedSearch = require(\u0026#39;modules/advanced-search\u0026#39;).component Components and state This is something that took us a while to figure out. I think that you should always keep display components stateless. You can usually use React stateless functional components for this.\nOf course you\u0026rsquo;ll need also somehow to map store state to your UI. This should be done inside separate, stateful components. These should not mess with presentation. They should pass all their state to child components by props, eventually select which components to render based on state, nothing more. How things look is totally up to stateless components.\nIt\u0026rsquo;ll make your components really easy to test and they\u0026rsquo;ll be as reusable as possible. Without it you\u0026rsquo;ll be constantly fighting with stores mocking, just to test if divs rendered properly and that data is displayed where it should. Managing state along with presentation also makes your components hard to reuse. Not to mention that components messing with state and presentation at once can get large.\nMost of the time you should be able to generate these stateful wrappers or they will be very simple to write and test.\nTo be honest, we still have quite a few these stateful display components in our project waiting to be refactored, but it\u0026rsquo;s not something I recommend to have. It\u0026rsquo;s painful to make any changes in them.\nTesting We don\u0026rsquo;t use Jest for testing. In my opinion mocking everything implicitly is not a very good idea. After a while you don\u0026rsquo;t know what\u0026rsquo;s mocked and what\u0026rsquo;s not.\nUpdate: As of version 15.0 Jest no longer mocks things by default. Haven\u0026rsquo;t used it very much, but I think it might be worth to try it out now.\nAll our components receive dependencies via props, so standard Jasmine/Karma testing suite is totally enough. We mock things and create spies as needed using plain JS and Jasmine. Every time we needed something more meant it was rather a design issue than library limitation (components trying to do too much or too big/nested data structures).\nEven stores are passed to stateful components using props. This way we can easily mock them or use real objects when needed and discard them afterwards. No ugly shared state that might not be cleaned up and break tests.\nAlso our stores and backend APIs use factories that get all outside dependencies from parameters:\nmodule.exports = { create: createSearchStore, default: createSearchStore(defaultDispatcher, backendApi.default), events } Again, mocking and cleaning up is very easy.\nImmutability You should keep all your data immutable. You can use things like Immutable.js and it will probably work pretty well (despite it\u0026rsquo;s sometime awkward APIs), but just not modifying anything in place and using Object.assign or similar utils instead will make your life much easier. Especially when you need to implement shouldComponentUpdate from time to time.\nThe lowest level you mutate anything should be store itself. Everything below should never be touched.\nDon\u0026rsquo;t be worried about performance. JS VMs are pretty damn fast when creating and destroying a lot of objects.\nState changes and async calls We keep all sate changing code inside stores. Stores mutate themselves in response to actions and action creators have no idea about internals of store. This also means that async calls are done inside store and do not call any actions. They just update internal state and fire regular store events.\nIt works pretty well for us despite being little controversial. Some guys recommend to do these things in actions and make your stores a dumb bag for data. This might also work well, especially when you need to intercept async calls globally and modify some other part of application state. But please decide which one you like more and stick to it. Having some of it in actions and some in stores will probably give you headache.\nAll business logic in stores makes our action creators dumb, so we usually generate them from simple object with events. If you go with second solution you can probably generate most of your stores.\nAlso it means we sometimes need to make stores listen to stores. We have few of these with no issues. It looks and works much better than \u0026lsquo;hacky\u0026rsquo; waitFor in dispatcher API.\nRedux? After some time it appeared to me, that some of my conclusions are quite similar to what Dan Abramov is doing in Redux. It\u0026rsquo;s little bit more extreme with its single store and pure functions, but after testing it for some time I think it\u0026rsquo;s really good. So another good idea for your next app might be to simply use Redux. It\u0026rsquo;s something I\u0026rsquo;m probably going to do in mine.\n","permalink":"https://marekpiechut.github.io/post/2016-09-02-react-lessons-learned/","summary":"It\u0026rsquo;s almost a year now since I\u0026rsquo;m working with React and Flux. After all this time I think I\u0026rsquo;ve learned a lot about how to build React apps, so they are easy to test, maintain and that components can be easily reused.\nHere are some of my most important conclusions. I think following them will make your life really easier, especially if you\u0026rsquo;re just starting with it.\nIs it worth it?","title":"One year with React"},{"content":"Sharing database between application services is one of worst and most common anti-patterns you can still see around. It will hurt you in so many ways, yet it looks so nice and easy at the beginning.\nGoing fast like crazy In the beginning it looks like it makes you go fast. You can get all data in application by just querying same database. Every modification you do is immediately visible in other services. You get atomicity with database transactions. No need to really handle things like network failures, endpoint unavailability, timeouts, firewalls etc. You just build a nice old monolith, only deployed on many machines. Development is going fast and easy. Your developers throw new features like machine gun, but then\u0026hellip;\nYou release \u0026hellip;and after release, like after every release, some thing didn\u0026rsquo;t work out as well as you expected. It\u0026rsquo;s nothing big, just some minor things here and there, so you start fixing it.\nYou need to add new column to database. Simple stuff. DB migrations are ready and tested. Software is updated. Now time for deployment. You need to stop all services\u0026hellip; Wait! What? All? Yes, all, you need to run DB migrations and you don\u0026rsquo;t want to run them live.\nAnd so you have lost one of advantages of dividing your application into many independent services. You can\u0026rsquo;t update them separately. Every database change will now require shutdown of all services. Users of your app and APIs will have to handle it. You\u0026rsquo;ll need to notify them upfront of any update and schedule them taking their\u0026rsquo;s schedule into consideration. Of course you would also need to do it when stopping single service, but net effect is much smaller. It\u0026rsquo;s not that bad though, you leave it as is and live on. Then you need to do some\u0026hellip;\nIncompatible changes to your database schema Changes that will require update of database queries. Maybe it\u0026rsquo;s a change required by performance degradation after database got some real data, maybe new requirements in one of services or you simply found a way to do something better.\nAgain, at first, everything looks easy. You update software. Database schema evolution scripts are ready. You\u0026rsquo;re a professional, so everything is well tested and ready to be deployed. Operations do the update and\u0026hellip; some other service starts failing. It\u0026rsquo;s not totally dead, just one of the endpoints fail. Of course you think its related to the last update. Last change is always first suspect. But first you need to get system up and running again.\nMaybe you can do a quick revert of previous update. But it\u0026rsquo;s not always easy. Sometimes reverting database migration is very hard and takes a lot of time. Hopefully you have prepared revert scripts before going with the update. If not, then this is probably moment when you learn that they are needed. Learn the hard way\u0026hellip;\nSometimes update to queries in second service is straightforward and you can do it in few minutes, but when you add whole release cycle it might take few days. Few days of unplanned downtime is quite a lot. I wouldn\u0026rsquo;t count on high bonuses afterwards.\nAfter getting things back online you start to investigate. Things might be much easier if your services communicated via some network API. You could check logs or network traffic on updated service and see which client failed, maybe even get some error codes. Now you need to slowly check all changes that went into release (db/query updates might be only one thing on huge list of change log items).\nAfter some time one of team members recalls, that some other service also uses same database table you have changed and might be affected. You find guilty service, fix code and prepare release.\nEverything runs smoothly afterwards, but your manager expects you to create some mitigation plan to make such incidents less probable in the future. What can you do? You make sure, that in the future you release, test and install all services at once. Now release process is also huge and painful.\nAnd these other minor things Remember that every update to you database causes downtime of all of your services. Depending on how you count your SLAs, it might cost you much. Every unplanned downtime or performance degradation of database system will make all your services unavailable. Maintenance tasks you periodically run on database system, software and hardware updates, security patches, etc. All these will have to be done at the same time for all services.\nI\u0026rsquo;ts also worth noticing, that probably every service database usage is somewhat different. Some of them mostly read, some write. Some use huge reporting queries running for very long time, some do simple CRUD operations based on IDs. There\u0026rsquo;s no way you\u0026rsquo;ll be able to optimize schema and indexes for all of them at the same time. Probably you\u0026rsquo;ll end with a compromise that\u0026rsquo;s equally bad for all usages. Good luck with query performance investigation. It will be hard.\nGoing back to software updates, they not always help with performance. Sometimes after updating database you see huge drop in response times due to slow queries. Usually these can be fixed with small changes in queries or messing with indexes, but do you want to do them all at once? Wouldn\u0026rsquo;t it be easier to do it one service after another?\nFinally you might get an idea, that maybe some other type of database might be more accurate for particular service. How are you going to do it if it shares database with some other service?\nAll this looks bad, but you might have got\u0026hellip;\nEven worse idea To share not only your database, but also entity classes as a common dependency. If shared database cause pain, this is an inner ring of hell.\nConsider a case, when you update entity API because it makes using it easier in one of services. Of course you version your software and all other services can simply continue using old version. No need to update anything now. But then you need to also do some changes in the very same entity class for second service. How can you do it without first applying changes made for first in second code? There\u0026rsquo;s no way you can isolate these changes. Only way is to fork entity project and keep two separate branches of code, but then you end up with separate entity dependencies.\nAgain, no way you can make API nice and cohesive for both services. You end up with bag of classes with unused APIs in both projects and no easy way to update them separately.\nWhat you\u0026rsquo;ll probably start doing in the future is keeping entity dependency at the same version for all services, so you\u0026rsquo;re sure everything is consistent. You\u0026rsquo;ll start to introduce changes just to conform to new API of entities, without any functional change. It also means you do releases, that do not change anything, just to make sure that entities are in the same versions in all running services.\nOf course you also end up using same technologies just to reuse entities, even if it does not make any sense.\nDatabase is an integral part of service Remember, database is the most intimate part of your service. It\u0026rsquo;s the part you need to keep most secured and do not allow anybody in. Even these corporate reporting tools. They also should go trough API of your services or have separate database that is filled by your application. It\u0026rsquo;s always better to add some APIs or even whole service to support what outside world needs, or you\u0026rsquo;ll end with sealed schema that\u0026rsquo;s unchangeable for any reason.\nYour services should have well defined APIs you wish to support and are going to keep compatible as long as it makes sense. You never know. Maybe you end up changing your database to bunch of CSV files in the end if it fits your needs better. For sure you don\u0026rsquo;t want to search for everybody using your database before you can do any update to schema.\n","permalink":"https://marekpiechut.github.io/post/2016-07-31-shared-database-pain/","summary":"Sharing database between application services is one of worst and most common anti-patterns you can still see around. It will hurt you in so many ways, yet it looks so nice and easy at the beginning.\nGoing fast like crazy In the beginning it looks like it makes you go fast. You can get all data in application by just querying same database. Every modification you do is immediately visible in other services.","title":"Sharing database between services will hurt you"}]